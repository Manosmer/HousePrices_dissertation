---
title: "Dissertation on Housing Market Analysis"
subtitle: "Best Possible Regression"
author: "Emmanouil Mertzianis, Student ID: 2600474"
date: "10/10/2021"
output:
  pdf_document:
    latex_engine: xelatex
    fig_caption: yes
    number_sections: yes
    toc: true
    toc_depth: 2
    includes:
      in_header: my_header.tex
fontsize: 12pt
bibliography: references.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
```

```{r libraries}
library(knitr)

library(skimr)
library(tidyverse)
library(moderndive)
library(GGally)
library(glmnet)
library(corrplot)
```



\newpage


# Introduction {#Intro}
The housing market is an important factor of the global economy both directly as a form of investment and indirectly, because of its strong ties with the overall robustness of the system and its crisis recovery ability [https://www.imf.org/en/News/Articles/2015/09/28/04/53/sp060514]. As Min Zhu, Deputy Managing Director of IMF, stated, _"IMF research shows that of the nearly 50 systemic banking crises in recent decades, more than two thirds were preceded by boom-bust patterns in house prices"_ [https://www.imf.org/en/News/Articles/2015/09/28/04/53/sp060514].

The primary objective of this paper is to find the best possible regression model to accurately _predict_ the sale price of houses. It is desirable that the model allows for a meaningful interpretation, however our main goal is prediction. For this purpose, 500 observations of house sales in the last six months have been collected by a firm, with measurements on the following variables:

* **elevation**: Elevation of the base of the house
* **dist_am1**: Distance to Amenity 1
* **dist_am2**: Distance to Amenity 2
* **dist_am3**: Distance to Amenity 3
* **bath**: Number of bathrooms
* **sqft**: Square footage of the house
* **parking**: Parking type
* **precip**: Amount of precipitation
* **price**: Final House Sale Price

The rest of the material in this paper is arranged as follows: in the next section we explore the available variables and their relationships via numerical and graphical summaries to make educated decisions when constructing our models. The following section contains the formal design, application and assessment of statistical regression models using our data with the ultimate goal of finding the best possible predictive model. Finally, we summarise our findings and we note ideas that could extend the analysis of our paper.

~~ TODO ~~


# Exploratory Data Analysis {#EDA}
```{r loadData}
houseprices <- read.csv("~/Desktop/Glasgow Project/housing.csv")

houseprices$bath <- as.factor(houseprices$bath)
houseprices$parking <- as.factor(houseprices$parking)
```

Before we start searching for the best regression model through formal data analysis and model fitting, it is important to explore our data through numerical and graphical summaries. This will allow for a better understanding of the patterns in and the structure of our data and it will enable us to make educated decisions during model fitting. For this purpose, we start by exploring each variable individually and, then, we focus on the relationships between the variables with emphasis on the ones related to the sale price, which is the variable of interest.


## Exploring variables individually
```{r skimdata, }
skimmed <- houseprices %>% 
  skimr::skim() %>%
  skimr::focus("Unique lvls"=factor.n_unique, Mean=numeric.mean, "SD"=numeric.sd, Min=numeric.p0, "25%"=numeric.p25, Median=numeric.p50, "75%"=numeric.p75, Max=numeric.p100)


# counts for the categorical variables
bathfreqTable <- houseprices %>%
  count(bath) %>%
  spread(key="bath", value="n")

bathfreq <- ""
for(bathnum in colnames(bathfreqTable)) {
  if(bathfreq == "") {
    bathfreq <- paste(bathnum,bathfreqTable[bathnum], sep=": ")
  } else {
    bathfreq <- paste(bathfreq, paste(bathnum,bathfreqTable[bathnum], sep=": "), sep=", ")
  }
}

parkingfreqTable <- houseprices %>%
  count(parking) %>%
  spread(key="parking", value="n")

parkingfreq <- ""
for(parkingtype in colnames(parkingfreqTable)) {
  if(parkingfreq == "") {
    parkingfreq <- paste(parkingtype,parkingfreqTable[parkingtype], sep=": ")
  } else {
    parkingfreq <- paste(parkingfreq, paste(parkingtype,parkingfreqTable[parkingtype], sep=": "), sep=", ")
  }
}

## The 2 tables ##
# Factors table
skimr::yank(skimmed, "factor") %>%
  rename("Variable"=skim_variable) %>%
  mutate("Counts"=c(bathfreq, parkingfreq)) %>%
  kable(caption="\\label{tab:statsCatOriginal} Summary statistics for the categorical variables in the initial data set.")

# Numerics table
skimr::yank(skimmed, "numeric") %>%
  rename("Variable"=skim_variable) %>%
  kable(caption="\\label{tab:statsNumOriginal} Summary statistics for the numerical variables in the initial data set.")

```

As a first step, we are interested in the summary statistics of the individual numerical and categorical variables in our data. The tables \ref{tab:statsCatOriginal} and \ref{tab:statsNumOriginal} contain useful statistics about the variables, prior to making any alterations to the original data set. We note that there are no missing values for any of our variables in the data.

Regarding the categorical variables, we observe that there are five and four unique levels for the categorical variables _"bath"_ and _"parking"_, respectively. Table \ref{tab:statsCatOriginal} shows that most of the sale entries refer to houses with two baths or an "open" type parking. However, the most important observation to note here is a single entry with 63 bathrooms, which is exceedingly higher than all the rest observations in our data that are limited to just 4 bathrooms at maximum. Such an observation is likely to be an outlier and the exploratory analysis to follow further underpins this assumption.

Table \ref{tab:statsNumOriginal} shows statistics about the numerical variables. It becomes apparent that the numerical variables are measured in different numerical scales with differences in the magnitude of their values. In terms of magnitude and standard deviation in ascending order:

* _"elevation"_ presents the smallest values that do not exceed the value of 47 and the smallest standard deviation.

* _"precip"_ and _"sqft"_ come second and third, respectively, with the latter having almost double the standard deviation of the former.

* The three variables representing the distance from three chosen amenities (i.e. _dist_am1_, _dist_am2_ and _dist_am3_) exhibit almost the same standard deviation. However, it seems that the $75^{th}$ percentile of _"dist_am1"_ is, relatively close to the $25^{th}$ percentile of _"dist_am2"_, while the $25^{th}$ percentile of _"dist_am3"_ is a bit higher than the median of _"dist_am2"_. This could indicate that, on average, the distance of houses from _"Amenity 1"_ could be significantly smaller than that from _"Amenity 2"_ and equally for the distances of houses from the _"Amenity 2"_ and _"Amenity 3"_.

* The numerical scale and the standard deviation of _"price"_ are the largest among all numerical variables. Also, it is interesting to point out that there exists a high outlier in _"price"_, even relative to its large magnitude, as it is `r (max(houseprices$price) - mean(houseprices$price))/sd(houseprices$price)` times the standard deviation greater than the mean value. The boxplot in figure \ref{priceBoxplot} further illustrates the extreme outlier in _"price"_.

```{r priceExtremeOutlierBoxplot, out.width="50%", fig.align='center', fig.cap="\\label{priceBoxplot} Boxplot of sale price."}
boxplot(houseprices$price, xlab="Sale Price")
```


Table \ref{tab:ExtremeOutlier} focuses on the aforementioned extreme outlier in _"price"_. The table reveals that the outlier (that is, the `r which.max(houseprices$price)`$^{th}$ observation) contains extreme values in other variables as well. Specifically, that same observation is the one related to the 63 bathrooms, which we have already noted as a possible extreme value, and through further exploration we can show that its value of `r max(houseprices$sqft)` square feet is also extremely high. Those findings suggest that observation `r which.max(houseprices$price)` could have probably come from a different population compared to the rest of the observations in the data. In any case, we lack enough data between this extreme observation and the rest ones, to the point that any model fitting with this outlier included would result in speculating after some range of values and would probably lead to a heavily influenced model. Therefore, we conclude that **we have enough evidence to support our decision on removing observation `r which.max(houseprices$price)` before we move on any further**.


```{r extremeOutlier}
houseprices[which.max(houseprices$price),] %>%
  kable(caption="\\label{tab:ExtremeOutlier} The extreme observation in the variable _'price'_.")
```

```{r}
# removing the extreme high outlier from the data set
houseprices <- houseprices[-which.max(houseprices$price),]
houseprices$bath <- as.factor(as.character(houseprices$bath))
```



```{r skimOutlierRemoved}
skimmed <- houseprices %>% 
  skimr::skim() %>%
  skimr::focus(Mean=numeric.mean, "SD"=numeric.sd, Min=numeric.p0, "25%"=numeric.p25, Median=numeric.p50, "75%"=numeric.p75, Max=numeric.p100)

# Numerics table
skimr::yank(skimmed, "numeric") %>%
  rename("Variable"=skim_variable) %>%
  kable(caption="\\label{tab:statsNumOutlierRemoved} Summary statistics for the numerical variables after removing the extreme outlier.")

```

After removing the outlier, our conclusions about the variables _"elevation"_, _"dist_am1"_, _"dist_am2"_, _"dist_am3"_ and _"precip"_ are similar to the ones we derived earlier. However, we observe a significant drop in the maximum value of _"sqft"_ along with a significant decrease in its standard deviation, which has now become relatively close to that of _"precip"_. Also, the maximum value and the standard deviation of _"price"_ incurred a large drop.

The boxplots in figure \ref{fig:boxplotAllOutlierRemoval} present graphically the already discussed differences in the magnitude and the variation between the numerical variables, by gradually removing variables from plot to plot. **A closer view using histograms in figure \ref{fig:histogramsAllOutlierRemoval} reveals that the sample distribution of all numerical variables in the data resembles that of a sample coming from a Normal Distribution.**



```{r boxdata, fig.align='center', out.width=".49\\linewidth", fig.width=2, fig.height=2, fig.show='hold', fig.cap="\\label{fig:boxplotAllOutlierRemoval} Boxplots on all numerical variables without the extreme outlier."}

houseprices %>%
  select(-bath, -parking) %>%
  gather(key="varname", value = "value") %>%
  ggplot(mapping = aes(x = varname, y = value)) +
  geom_boxplot() +
  xlab(NULL) + ylab("Values") +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))

houseprices %>%
  select(-bath, -parking, -price) %>%
  gather(key="varname", value = "value") %>%
  ggplot(mapping = aes(x = varname, y = value)) +
  geom_boxplot() +
  xlab(NULL) + ylab("Values") + 
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))

houseprices %>%
  select(elevation, precip, sqft) %>%
  gather(key="varname", value = "value") %>%
  ggplot(mapping = aes(x = varname, y = value)) +
  geom_boxplot() +
  xlab(NULL) + ylab("Values") + 
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))
```


```{r histograms, fig.align='center', out.width=".49\\linewidth", fig.width=1.5, fig.height=1.5, fig.show='hold', fig.cap="\\label{fig:histogramsAllOutlierRemoval} Histograms on all numerical variables without the extreme outlier. The red dashed line represents the sample mean of each variable."}

numericalVariables<- houseprices %>%
                      select(-bath, -parking)


numericalVariables %>%
  ggplot(aes(x = price)) + geom_histogram(bins = 100) + geom_vline(xintercept = mean(numericalVariables$price), colour="red", lty = 2) + 
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))

numericalVariables %>%
  ggplot(aes(x = sqft)) + geom_histogram(bins = 80) + geom_vline(xintercept = mean(numericalVariables$sqft), colour="red", lty = 2) + 
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))

numericalVariables %>%
  ggplot(aes(x = elevation)) + geom_histogram(bins = 80) + geom_vline(xintercept = mean(numericalVariables$elevation), colour="red", lty = 2) + 
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))

numericalVariables %>%
  ggplot(aes(x = precip)) + geom_histogram(bins = 80) + geom_vline(xintercept = mean(numericalVariables$precip), colour="red", lty = 2) + 
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))

numericalVariables %>%
  ggplot(aes(x = dist_am1)) + geom_histogram(bins = 80) + geom_vline(xintercept = mean(numericalVariables$dist_am1), colour="red", lty = 2) + 
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))

numericalVariables %>%
  ggplot(aes(x = dist_am2)) + geom_histogram(bins = 80) + geom_vline(xintercept = mean(numericalVariables$dist_am2), colour="red", lty = 2) + 
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))

numericalVariables %>%
  ggplot(aes(x = dist_am3)) + geom_histogram(bins = 80) + geom_vline(xintercept = mean(numericalVariables$dist_am3), colour="red", lty = 2) + 
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))

```


## Exploring relationships
The primary objective of this paper is to create the most accurate predictive model about _"price"_ using the rest of the available variables in the data. Therefore, it is of interest to explore the relationships of _"price"_ against the other variables. For this purpose, we first explore the relationships of _"price"_ against all other numerical variables and, then, we focus on the categorical variables. Finally, we analyse the relationships of _"price"_ against the numerical variables on different levels of the categorical variables.


### Numerical explanatory variables {#EDA:NEV}

Figure \ref{fig:scatterPriceAll} depicts the relationships of _"price"_ against all the rest numerical variables in the form of scatterplots with a simple linear regression line superimposed. From the scatterplots, we observe a random scattering of the data points across all values with no obvious patterns suggesting that there is little association between _"price"_ and any one of the other numerical variables. Additionally, the small slope of the superimposed regression lines along with the little correlation revealed in table \ref{tab:corrplot} show that there is no linear relationship between any of the numerical variables and _"price"_. However, we observe a high correlation between _"dist\_am3"_ - _"dist\_am1"_ and a moderate correlation between _"dist\_am1"_ - _"dist\_am2"_ and _"dist\_am2"_ - _"dist\_am3"_ indicating that there is possibly multicollinearity between the distance variables. 

A more detailed investigation on multicollinearity can be conducted by fitting linear regression models using each one of the distance variables as the response each time. The values of $R_{(adj)}^2$ reveal that we can explain 72.08% of the variability in _"dist\_am3"_ using the other two distance variables. Moreover, when we treat any of the other two variables as our response and modelling it with the other two, _"dist\_am3"_ is almost solely responsible for observing a high $R_{(adj)}^2$ value. In fact, when we use just _"dist\_am2"_ to model _"dist\_am1"_ we can explain just 19.26% of its variability in contrast to 63.27% when we use just _"dist\_am3"_.

In terms of _"price"_, the $R^2$ score from using just _"dist\_am3"_, **although very small**, is better than the one we obtain from using _"dist\_am1"_ and _"dist\_am2"_, together or alone. Also, the p-value for the coefficient estimate of _"dist\_am3"_ is 0.0502, which is very close to the 5% significance level we chose to use in this paper.

Finally we note that, before we decide on removing any of the distance variables in order to overcome multicollinearity, it is important to assess their relationship to _"price"_ from their interactions with the categorical variables in the data. The exploration of interactions is presented in subsection \ref{EDA:Inter}.


```{r, fig.align='center', out.width="0.49\\linewidth", fig.show='hold', fig.width=2, fig.height=2, fig.cap="\\label{fig:scatterPriceAll} Scatterplots of 'price' against all the rest numerical variables. The simple linear regression line is superimposed on the plots."}

houseprices %>%
  ggplot(mapping = aes(x = dist_am1, y = price)) + 
  geom_point(size = 0.8, alpha=.4, shape = 1) +
  geom_smooth(method = "lm", se = FALSE, lwd=.7)


houseprices %>%
  ggplot(mapping = aes(x = dist_am2, y = price)) + 
  geom_point(size = 0.8, alpha=.4, shape = 1) +
  geom_smooth(method = "lm", se = FALSE, lwd=.7)

houseprices %>%
  ggplot(mapping = aes(x = dist_am3, y = price)) + 
  geom_point(size = 0.8, alpha=.4, shape = 1) +
  geom_smooth(method = "lm", se = FALSE, lwd=.7)

houseprices %>%
  ggplot(mapping = aes(x = elevation, y = price)) + 
  geom_point(size = 0.8, alpha=.4, shape = 1) +
  geom_smooth(method = "lm", se = FALSE, lwd=.7)

houseprices %>%
  ggplot(mapping = aes(x = precip, y = price)) + 
  geom_point(size = 0.8, alpha=.4, shape = 1) +
  geom_smooth(method = "lm", se = FALSE, lwd=.7)

houseprices %>%
  ggplot(mapping = aes(x = sqft, y = price)) + 
  geom_point(size = 0.8, alpha=.4, shape = 1) +
  geom_smooth(method = "lm", se = FALSE, lwd=.7)
```

```{r, out.width="50%", fig.align='center', fig.cap="\\label{tab:corrplot} Correlation between all numerical variables."}
corrplot::corrplot(cor(houseprices %>% select(-bath, -parking)), method="number", type="upper")
```


### Categorical explanatory variables

Figure \ref{fig:boxplotsCat} depicts the sample distribution of _"price"_ for each level of _"bath"_ or _"parking"_. Regarding _"parking"_, we observe quite a significant overlap between the boxplots with small differences between them suggesting that there is little to no difference in the sale price for houses in different _"parking"_ categories. In contrast, we see that there is no overlap between the boxplots of _"price"_ in different _"bath"_ categories with the sale price actually increasing as the number of bathrooms increases. As it can be seen more clearly in figure \ref{fig:histPricebyBath}, the seemingly normally-distributed sample of _"price"_ is completely partitioned based on _"bath"_ into 4 non-overlapping chunks. Also, the p-values of the estimated parameters when using a linear regression model with _"bath"_ as our sole predictor are 0, while $R_{(adj)}^2$ reports that we can already explain 86.13% of the total variability in _"price"_. These findings indicate strongly that the categorical variable _"bath"_ is a significant predictor with a positive relationship with _"price"_.


```{r, out.width=".49\\linewidth", fig.align='center', fig.show='hold', fig.width=3, fig.height=3, fig.cap="\\label{fig:boxplotsCat} Boxplots of 'price' by 'bath' and by 'parking'."}
houseprices %>%
  ggplot(mapping = aes(x = bath, y = price)) +
  geom_boxplot()

houseprices %>%
  ggplot(mapping = aes(x = parking, y = price)) +
  geom_boxplot() + 
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))
```


```{r, fig.align='center', out.width="50%", fig.cap="\\label{fig:histPricebyBath} Histogram of 'price' coloured by 'bath'."}
ggplot(data=houseprices, mapping = aes(x=price, fill=bath)) +
  geom_histogram(bins=100)
```


### Interactions {#EDA:Inter}

Finally, it is important to investigate any interactions between categorical and numerical explanatory variables. Figure \ref{fig:scatterPriceAllByBath} shows the relationship of each numerical variable with _"price"_, on each _"bath"_ level, in the form of scatterplots with simple regression lines superimposed. On all plots, we observe four distinct layers of data corresponding to each _"bath"_ level. As we have already explained, these layers correspond to the full partitioning of the sample distribution of _"price"_ into four non-overlapping chunks when considering the variable _"bath"_. It is apparent that, regardless of the _"bath"_ category they belong to, the observations in the data extend fairly across the whole range of values on all numerical explanatory variables. 

Apart from _"precip"_, the nature of the relationship between the rest of the explanatory variables and _"price"_ is roughly the same on each level of _"bath"_ and the relationships do not seem to improve for any of the variables when we take _"bath"_ into consideration (as seen from figure \ref{fig:scatterPriceAllByBath} and the high p-values). In the case of the relationship between _"precip"_ and _"price"_, we observe a slightly positive trend on all levels of _"bath"_ except for the houses with one bathroom, where the relationship becomes negative. Therefore, the aforementioned relationship seems to improve and become important when considered under each _"bath"_ category. This fact is further supported by the p-values shown in table \ref{tab:precipBathFit} which are either close to the chosen 5% significance level or below it.

```{r}
get_regression_table(lm(price ~ precip*bath, data=houseprices)) %>%
  select(term, estimate, p_value, lower_ci, upper_ci) %>%
  kable(caption="\\label{tab:precipBathFit} The regression coefficient estimates, p_values and 95% C.Is from fitting the linear model with just an interaction between 'precip' and 'bath'.")
```


When we assess the same relationships under each category of _"parking"_, it can be shown that there is an almost complete overlap between the layers of data of the different _"parking"_ levels, with a random scattering of the data points and the p-values on those relationships are fairly large. This suggests that the single regression line model should be suitable for each relationship with _"price"_, with no difference in the slopes or the intercept terms among the categories (i.e. no interaction with _"parking"_). 

However, it appears that in the relationship of _"price"_ and _"dist\_am1"_ there is a significant difference in the coefficient estimate of _dist\_am1_ between the _"parking"_ categories "Covered"(baseline category) and "No Parking" and a significantly large p-value for the difference in the coefficient estimate between "Covered" and "Not Provided". This can be seen from both the slopes of the simple linear regression lines in figure \ref{fig:distAm1} and table \ref{tab:distam1ParkingFit}. Therefore, it is important to study formally the possible interaction of _"dist\_am1"_ and _"parking"_. 

Continuing with the results about multicollinearity in subsection \ref{EDA:NEV}, the interaction between _"parking"_  and _"dist\_am1"_ demonstrates a better $R_{(adj)}^2$ score and better p-values when modelling _"price"_ than _"parking"_, _"dist\_am3"_ or the interaction between the two. Therefore, **we decide on dropping variable _"dist\_am3"_ from our models**. This decision solves the problem of multicollinearity as the remaining two distance variables have a 0.44 correlation coefficient which is rather moderate. Also, the remaining two variables can explain 72.08% of the variability in _"dist\_am3"_, which suggests that _"dist\_am3"_ is fairly represented by the other two and we do not lose that much information in the data.

Finally, we note that other transformations we applied on the numerical explanatory variables and on _"price"_ did not seem to improve the relationships between them.


```{r, fig.align='center', fig.show='hold', out.width="0.49\\linewidth", fig.width=4, fig.height=3, fig.cap="\\label{fig:distAm1} 'Price' against 'dist\\_am1' on all 'parking' levels."}
houseprices %>%
  ggplot( mapping = aes( y = price, x = dist_am1, colour = parking) ) + 
  geom_point(shape = 1, size = 0.8) + 
  geom_smooth(method = "lm", se = FALSE, lwd = .7)
```

```{r}
get_regression_table(lm(price ~ dist_am1*parking, data=houseprices)) %>%
  select(term, estimate, p_value, lower_ci, upper_ci) %>%
  kable(caption="\\label{tab:distam1ParkingFit} The regression coefficient estimates, p_values and 95% C.Is from fitting the linear model with just an interaction between 'dist\\_am1' and 'parking'.")
```



```{r, fig.align='center', out.width="0.49\\linewidth", fig.show='hold', fig.width=4, fig.height=3, fig.cap="\\label{fig:scatterPriceAllByBath} Scatterplots of 'price' against all the rest numerical variables and a simple linear regression line superimposed, coloured by 'bath'."}
houseprices %>%
  ggplot(mapping = aes(x = dist_am1, y = price, colour=bath)) + 
  geom_point(size = 0.8, alpha=.4, shape = 1) +
  geom_smooth(method = "lm", se = FALSE, lwd=.7)

houseprices %>%
  ggplot(mapping = aes(x = dist_am2, y = price, colour=bath)) + 
  geom_point(size = 0.8, alpha=.4, shape = 1) +
  geom_smooth(method = "lm", se = FALSE, lwd=.7)

houseprices %>%
  ggplot(mapping = aes(x = dist_am3, y = price, colour=bath)) + 
  geom_point(size = 0.8, alpha=.4, shape = 1) +
  geom_smooth(method = "lm", se = FALSE, lwd=.7)

houseprices %>%
  ggplot(mapping = aes(x = elevation, y = price, colour=bath)) + 
  geom_point(size = 0.8, alpha=.4, shape = 1) +
  geom_smooth(method = "lm", se = FALSE, lwd=.7)

houseprices %>%
  ggplot(mapping = aes(x = precip, y = price, colour=bath)) + 
  geom_point(size = 0.8, alpha=.4, shape = 1) +
  geom_smooth(method = "lm", se = FALSE, lwd=.7)

houseprices %>%
  ggplot(mapping = aes(x = sqft, y = price, colour=bath)) + 
  geom_point(size = 0.8, alpha=.4, shape = 1) +
  geom_smooth(method = "lm", se = FALSE, lwd=.7)
```



# Model Fitting: Selecting the best possible regression model {#ModelFitting}

The **primary objective** of this paper is to search for and design the best possible _predictive_ regression model to accurately predict the sale price of a house based on the rest of its attributes. The main approach we chose for this problem is **best-subset selection on the full model via backward elimination**. 

For this purpose, we decided on assessing our models' performance and conduct variable selection based on the average **Mean Squared (Prediction) Error** calculated using 5-fold cross-validation(C.V.) on the training data:

$$ \hat{MSPE} = \frac{1}{n} \sum_{i=1}^n{(y_i - \hat{y_i})^2}$$

We achieve this by splitting our original data set into two subsets; training and test. The training set will be used for training and assessing the prediction performance(out-of-sample prediction) via 5-fold C.V. and the test set for calculating the expected prediction performance on the best model. The split we chose to use is:

* 450 observations for training and C.V., 
* 49 observations for assessment

The reason for choosing cross-validation instead of a simple training/validation/test split is to avoid the case of a bad split, especially with a small data set as ours. With K-fold C.V. we are able to use all of our data and calculate a more robust estimate of the average (expected) prediction error [book ESLII]. In practice, we have used both techniques and we have experienced more consistent results from cross-validation (even with 5, 6 or 10 folds) than from using the simple 3-subset split. That is, small changes in the training set or in the distribution of the splits resulted in almost the same final subset of predictors. Also, the choice of the number of folds to use is of great importance as it balances the bias-variance trade-off. However, a usual choice for that number is 5 or 10 folds [Breiman and Spector or Kohavi], which is the choice we made in this paper.

```{r datasetSplitting}
set.seed(1635863590) # set a fixed seed so we get the same results each time we knit


testIndices <- sample(1:499, 49)
trainingIndices <- (1:499)[-testIndices]

testSet <- houseprices[testIndices,]
trainingSet <- houseprices[trainingIndices,]


# 5-fold split for 5-fold cross-validation
k <- 5
trainSetLength <- nrow(trainingSet)
splitSize <- floor( (trainSetLength / k) + .5)

splits <- list()
splits[[1]] <- sample(1:trainSetLength, splitSize)
usedIndices <- splits[[1]]
for(i in 2:(k-1)) {
  splits[[i]] <- sample( ( 1:trainSetLength )[-usedIndices], splitSize )
  
  usedIndices <- c(usedIndices, splits[[i]])
}
splits[[k]] <- (1:trainSetLength)[-usedIndices]

```


Finally, we will consider moving beyond the Ordinary Least Squares method by smoothing the best regression model's OLS estimates using Ridge regression on its subset of predictors and see whether we can improve the prediction performance even further. Also, we will investigate whether variable selection via Lasso regression yields a more powerful model than backward elimination.


## Fitting the full model
In section \ref{EDA}, we discussed about the different available variables in our data and we explored numerically and graphically the relationships between them, especially with _"price"_. Based on our findings, we select as our starting model the linear, additive model on the original variables excluding _"dist\_am3"_ and including interactions between _"precip"_ - _"bath"_ and _"dist\_am1"_ - _"parking"_. Therefore, we assume the true relationship is:

$$ price_i =  \beta_0 + \beta_1\cdot{\mathbb{I}bath|2_i} + \beta_2\cdot{\mathbb{I}bath|3_i} + \beta_3\cdot{\mathbb{I}bath|4_i} +$$
$$\beta_4\cdot{\mathbb{I}parking|NoParking_i} + \beta_5\cdot{\mathbb{I}parking|NotProvided_i} + \beta_6\cdot{\mathbb{I}parking|Open_i} +$$
$$\beta_7\cdot{sqft_i} + \beta_8\cdot{elevation_i} + \beta_{9}\cdot{dist\_am2_i} +$$
$$\beta_{10}\cdot{precip_i} + \beta_{11}\cdot{\mathbb{I}bath|2_i}\cdot{precip_i} + \beta_{12}\cdot{\mathbb{I}bath|3_i}\cdot{precip_i} + \beta_{13}\cdot{\mathbb{I}bath|4_i}\cdot{precip_i} +$$
$$\beta_{14}\cdot{dist\_am1_i} + \beta_{15}\cdot{\mathbb{I}parking|NoParking_i}\cdot{dist\_am1_i} + \beta_{16}\cdot{\mathbb{I}parking|NotProvided_i}\cdot{dist\_am1_i} +$$
$$\beta_{17}\cdot{\mathbb{I}parking|Open_i}\cdot{dist\_am1_i}$$
$$+~\epsilon_i,~~~~\epsilon_i \stackrel{indep.}\sim N(0, \sigma^2), ~~~ i=1,...,450$$

, where the indicator variables(starting with $\mathbb{I}$) are equal to $1$ when the $i^{th}$ observation's corresponding categorical variable takes the relevant value(separated from the variable name with "|") and $0$ otherwise. The baseline categories are **1** for _"bath"_ and **Covered** for _"parking"_.


Table \ref{tab:FullModelFit} shows the OLS estimates of the full model's regression coefficients along with their p-value and 95% Confidence Interval(C.I.), when fitted on the whole training set. The reported p-values suggest that we have enough statistical evidence that the coefficients for the categories of _"bath"_ are significant, when considering all the predictors we used in the model and choosing a significance level of 5%. Furthermore, the absence of overlap between the 95% C.Is of the  _"bath"_ categories further underlines the importance of this predictor. 

Additionally, almost all of the reported p-values related to the interaction between _"precip"_ and _"bath"_ are either below or close to the chosen 5% threshold, suggesting that this interaction seems important for predicting _"price"_, considering all the other predictors we used in the model. All the rest estimates have a p-value that is greater than the chosen significance level or, equivalently, they have a 95% C.I. that includes the value of $0$ as a plausible value for the coefficient in the population's regression model.

```{r}
joinVariables <- function(variableVector) {
  if(length(variableVector) <= 0) stop("Variable vector must contain one or more elements.");
  
  if(length(variableVector) == 1) return(variableVector[1])
  
  result <- variableVector[1]
  for(var in variableVector[c(-1,-length(variableVector))] ) {
    result <- paste(result, var, sep=" + ")
  }
  
  return( paste(result, variableVector[length(variableVector)], sep=" + ") )
}

calculateMSPE <- function(model, dataset, responseVar) {
  # calculate MSPE on validation set
  return( mean( (dataset[[responseVar]] - predict(model, dataset))^2 ) )
}

calculateMSPECV <- function(variableVector, dataset, responseVar, splits) {
  # Calculation of average MSPE via K-fold cross-validation
  
  dataSetLength <- nrow(dataset)
  k <- length(splits)
  if(k <= 1) stop("Data set splits must be more than 1.")
  
  MSPE <- 0
  for(i in 1:k) {
    validSet <- dataset[splits[[i]], ]
    trainSet <- dataset[ (1:dataSetLength)[ -splits[[i]] ], ]
    
    model <- lm( paste(responseVar, joinVariables( variableVector ), sep=" ~ "), data=trainSet)
    MSPE <- MSPE + calculateMSPE(model, validSet, responseVar) 
  }
  
  return(MSPE / k)
}
```


```{r}
fullmodel_variables <- c("bath", "parking", "precip", "precip*bath", "dist_am1", "dist_am1*parking", "dist_am2", "sqft", "elevation")

fullModel <- lm( paste("price", joinVariables( fullmodel_variables ), sep=" ~ "), data=trainingSet)

TSS <- sum( (trainingSet$price - mean(trainingSet$price))^2 )
full_model_RSS <- sum( (trainingSet$price - fullModel$fitted.values)^2 )

get_regression_table(fullModel) %>%
  select(term, estimate, p_value, lower_ci, upper_ci) %>%
  kable(caption = "\\label{tab:FullModelFit} The estimated regression coefficients along with the corresponding p-values and 95% Confidence Intervals from fitting the full model on the training set.")

fullModelAverageMSPE <- calculateMSPECV(fullmodel_variables, trainingSet, "price", splits)
```

Lastly, we can calculate the average Mean Squared Prediction Error of the full model via 5-fold cross-validation, which is equal to $\hat{MSPE}=$ `r fullModelAverageMSPE`. This metric will be used for performing variable selection as described in the following subsection.



## Variable Selection via Best-Subset Selection {#BestSubset}

```{r variableSelectionFunctions}
selectVariables <- function(variableVector, trainSet, validSet, responseVar) {
  removedVariables <- c() # contains removed variables in order
  MSPEs <- c() # contains MSPE values in order
  
  
  fullModel <- lm( paste(responseVar, joinVariables( variableVector ), sep=" ~ "), data=trainSet)
  
  # remove each variable at a time and calculate MSPE
  minMSPE <- calculateMSPE(fullModel, validSet, responseVar)
  removedVar <- 0
  bestFound <- FALSE
  while(!bestFound & length(variableVector) > 1) {
    bestFound <- TRUE # assume the best model is the one found from the previous iteration
    
    for(i in seq_along( variableVector )) {
      model <- lm( paste(responseVar, joinVariables( variableVector[-i] ), sep=" ~ "), data=trainSet)
      currentMSPE <- calculateMSPE(model, validSet, responseVar)
      
      if(minMSPE >= currentMSPE) {
        bestFound <- FALSE # if something better has been found, best model has not been found yet
        minMSPE <- currentMSPE
        removedVar <- i
      }
    }
    
    if(!bestFound) {
      # some variable has indeed been removed
      removedVariables <- c(removedVariables, variableVector[removedVar])
      MSPEs <- c(MSPEs, minMSPE)
      variableVector <- variableVector[-removedVar]
    }
  }
  
  return(list(removedVariables = removedVariables, MSPEs = MSPEs, bestModel = variableVector))
}


## Cross-validation ##
selectVariablesCV <- function(variableVector, trainSet, responseVar, splits) {
  # Variable Selection via backward elimination using K-fold CV to calculate the MSPE
  
  if(length(splits) <= 1) stop("The number of splits in Cross-validation must be greater than 1.")
  
  ### RESULTS ###
  removedVariables <- c() # contains removed variables in order
  MSPEs <- c() # contains MSPE values in order
  
  
  ### Full model average MSPE ###
  minMSPE <- calculateMSPECV(variableVector, trainSet, responseVar, splits)
  
  # remove each variable one at a time and calculate MSPE
  removedVar <- 0
  bestFound <- FALSE
  while(!bestFound & length(variableVector) > 1) {
    bestFound <- TRUE # assume the best model is the one found from the previous iteration
    
    for(i in seq_along( variableVector )) {
      currentMSPE <- calculateMSPECV(variableVector[-i], trainSet, responseVar, splits)
      
      if(minMSPE >= currentMSPE) {
        bestFound <- FALSE # if something better has been found, best model has not been found yet
        minMSPE <- currentMSPE
        removedVar <- i
      }
    }
    
    if(!bestFound) {
      # some variable has indeed been removed
      removedVariables <- c(removedVariables, variableVector[removedVar])
      MSPEs <- c(MSPEs, minMSPE)
      variableVector <- variableVector[-removedVar]
    }
  }
  
  return(list(removedVariables = removedVariables, MSPEs = MSPEs, bestModel = variableVector))
}

```


Although the full model is a good starting point to consider all predictors that could be possibly related to the response variable, it is almost certain that we have included excessive, unnecessary complexity that leads to overfitting. Usually, such a model is very flexible, lacks stability (high variance) and captures sample-specific features that are not generalisable to other data from the same population (noise). In other words, this kind of models excel in estimating on their training data set, but fail to demonstrate a good performance in out-of-sample prediction. 

Since our goal is to construct the best predictive model, we wish to minimise the average $\hat{MSPE}$ we obtain from cross-validation as much as possible and the method we chose to achieve this is **best-subset selection via backward elimination on the full model**. This iterative method starts by removing one variable at a time from the full model and calculating the average $\hat{MSPE}$ after each removal by performing 5-fold C.V. on the training set, until it has removed each variable once. Then, the method identifies the variable that resulted in the best(lowest) $\hat{MSPE}$ value after its removal and updates the model such that it does not contain this variable any longer. The method continues by seeking the next best variable to remove on the updated model. This process is repeated until either there is only one independent variable left or no more improvement can be achieved. 

In simple words, we decrease the overall complexity of our model by gradually removing unnecessary variables, while we continuously monitor the improvement of the $\hat{MSPE}$ averaged across the 5 folds of our data, until we reach the best possible value. It is easy to realise that this method belongs to the family of greedy algorithms as it consists of a series of consecutive, independent and non-reversible steps at which the optimal solution is chosen each time.


```{r variableSelection}
# perform K-fold cross-validation
variablesSelectionResults <- selectVariablesCV(fullmodel_variables, trainingSet, "price", splits)

# table of VarSel. first row contains full model MSPE
selectionTable <- c("full model", fullModelAverageMSPE)
selectionTable <- rbind(selectionTable, cbind(variablesSelectionResults$removedVariables, variablesSelectionResults$MSPEs))
selectionTable <- selectionTable[-nrow(selectionTable),] # precip is shown as removed, but precip*bath exists in the model

selectedModel <- lm(paste("price", joinVariables(variablesSelectionResults$bestModel), sep=" ~ "), data=trainingSet)
selectedModel_RSS <- sum( (trainingSet$price - selectedModel$fitted.values)^2 )

data.frame("removedVars"=selectionTable[,1], "MSPE"=selectionTable[,2]) %>%
  kable(col.names = c("Removed variable", "cross-validated $\\hat{MSPE}$ after removal"),
        caption="\\label{tab:variableSelection} Results from performing variable selection via backward elimination on the full model. The left column contains the removed variables in the order of removal, while the right one shows the average $\\hat{MSPE}$ from 5-fold C.V. after the removal of the variable on the left. The full model with its average $\\hat{MSPE}$ is displayed in the first row for reference.", align="c")
```

Table \ref{tab:variableSelection} shows the process of applying the aforementioned variable selection method on our full model. Apparently, the method managed to reduce the average $\hat{MSPE}$ down to `r calculateMSPECV(variablesSelectionResults$bestModel, trainingSet, "price", splits)` from the initial, full model's value of `r fullModelAverageMSPE`. As a result, we now update our assumption about the true relationship between _"price"_ and our predictors in the population such that:
$$ price_i =  \beta_0 + \beta_1\cdot{\mathbb{I}bath|2_i} + \beta_2\cdot{\mathbb{I}bath|3_i} + \beta_3\cdot{\mathbb{I}bath|4_i} +$$
$$\beta_4\cdot{precip_i} + \beta_5\cdot{\mathbb{I}bath|2_i}\cdot{precip_i} + \beta_6\cdot{\mathbb{I}bath|3_i}\cdot{precip_i} + \beta_7\cdot{\mathbb{I}bath|4_i}\cdot{precip_i}$$
$$+~\epsilon_i,~~~~\epsilon_i \stackrel{indep.}\sim N(0, \sigma^2), ~~~ i=1,...,450~~$$ 
[(4.2.1)]{#VSModel}

The results from fitting model [4.2.1](#VSModel) on the training data set are shown in table \ref{tab:varSelModel}. The reported p-values indicate that, accounting for all of the variables we used in our model, the categorical variable _"bath"_ and the interaction between precipitation and _"bath"_ seem to be significant predictors in terms of the sale price of a house. More specifically, our fitted model estimates that:

* **for houses with 0 precipitation**, the expected sale price of a house with just one bathroom is, on average, 288833.246. This base price increases, on average, by 98592.790, 298359.921 or 431877.270 for 2, 3 or 4 bathrooms, respectively. This gradual increase in the price as we increase the number of bathrooms combined with the non-overlapping 95% C.I. for the OLS estimates of the price differences between the _"bath"_ categories show a significant positive relationship between the two variables and agree with our findings in section \ref{EDA}.

* **for every 1 unit increase in precipitation**, the expected sale price of a house decreases, on average, by 50.454 for houses with 1 bathroom. However, the expected price seems to increase, on average, by 18.277, 3.206 or 39.697 per each unit increase in precipitation when the houses have 2, 3 or 4 bathrooms, respectively. The p-values of the estimates suggest that there is a statistically significant difference in the slopes of _"precip"_ between the categories of 1 and 2 bathrooms. Although we do not have enough evidence of a significant difference in the slopes between the rest of the categories, the reported p-values are very small(<10%) and some are close to 5%.

It is, also, worth mentioning that $R_{(adj)}^2$, which takes the complexity of the used model into account, improves from the full model's value of `r 1 - ( full_model_RSS/(nrow(trainingSet)-19) )/( TSS/(nrow(trainingSet)-1) )` to `r 1 - ( selectedModel_RSS/(nrow(trainingSet)-8) )/( TSS/(nrow(trainingSet)-1) )`, suggesting that the predictors in model [4.2.1](#VSModel) are able to explain 86.23% of the variation in the sale price.

```{r}
get_regression_table(selectedModel) %>%
  select(term, estimate, p_value, lower_ci, upper_ci) %>%
  kable(caption="\\label{tab:varSelModel} The final model fitted on the training set after performing best-subset selection.")
```

However, the interpretation of model [4.2.1](#VSModel) and its application in predicting sale prices are meaningless if the assumptions we made prior to fitting our model are violated. In reality, there is no formal way of checking the validity of those assumptions. However, the plots of the residuals in figure \ref{fig:VSassumptions} are commonly used in order to check the assumptions of our multiple linear regression model [4.2.1](#VSModel), as described in the book _"Linear Models with R"_ by J.J. Faraway[book]. 

The "Residuals vs. Fitted" plot above shows 4 distinct vertical groups of data points. This behaviour is expected as our model relies heavily on the 4 disjoint _"bath"_ categories to calculate the fitted price, while precipitation plays a smaller role in explaining the remaining variability. Also, the plot depicts a random and even scattering of the data points above and below the horizontal 0 line with no obvious patterns suggesting that the assumption that the errors' mean is 0 holds and that there is no significant non-random structure left unexplained in our data. Plots of residuals against the excluded predictors further support this claim, as they show a random scattering of the data points without any obvious patterns. 

Finally, we observe a fairly equal scattering of the points about the 0 line across the range of the fitted values which indicates that the assumption of constant variance(homoscedasticity) is valid. One could argue that there is smaller variability observed for the residuals of the first and last column in the plot. However, it is important to consider that the data set contains remarkably fewer observations for the categories of 1 or 4 bathrooms, which might be the reason behind this seemingly different variability. 

```{r, out.width=".49\\linewidth", fig.cap="\\label{fig:VSassumptions} Plots of the residuals from fitting model 4.2.1 on the training set.", fig.show='hold', fig.height=4, fig.width=4}
plot(selectedModel, which=1, sub.caption="")
plot(selectedModel, which=2, sub.caption="")
plot(selectedModel, which=5, sub.caption="")

ggplot(mapping=aes(x=selectedModel$residuals)) + 
  geom_histogram(bins=50) + xlab("Residuals")
# bins 26
```


Using the "Normal Q-Q" plot, we can check the assumption of normally distributed errors. The plot exhibits a light and a heavy tail and it does not have the _ideal_ shape. However, given the fact that we are modelling real-life data, it is reasonable to think that the points follow the dashed line adequately and the assumption holds. The same can be inferred from the supplemental histogram of residuals, which resembles the bell shape of the Normal distribution but not perfectly.

Lastly, we wish to look for any influential observations that we might need to discard as they could incorrectly draw the direction of the relationship towards them. Although the rest of the plots report consistently the observations "32" and "438" as possible outliers, the plot of residuals against leverage shows no observations within the contour lines of Cook's distance. Therefore, we choose not to remove any additional observations.


## Regularised regression with Ridge and Lasso

As mentioned in book [ESLII], "Best-Subset Selection" can be considered as a _discrete_ technique, in the sense that complexity reduction is achieved by choosing to either include a variable in the model or exclude it from it. 

However, there are methods that provide a _continuous_ and more flexible complexity reduction. The estimated parameter values from fitting the model using the least squares approach can be seen as the weighted influence of each predictor on the predicted outcome. As a result, there is opportunity to decrease the absolute value of those parameters in order to reduce the influence of some predictors(_smoothing_), thus resulting in a _continuous_ method of complexity reduction without necessarily removing the included predictors completely. This idea leads to "Regularised Regression" and both **Ridge** and **Lasso** belong to this family of regression techniques. 

In the Ordinary Least Squares(OLS) approach, which is the typical linear regression technique and the one we used so far in this paper, we want to estimate the regression parameters in such a way that the resulting regression line has in total the closest distance possible from the data points, given the selected predictors. This implies the use of a loss function that calculates the total discrepancy between the estimated line and the data points, which allows for finding the optimal solution through loss minimisation. Assuming that the parametric form of the chosen model is correct(true), the OLS estimates are unbiased[ESLII]. This means that, if we could fit the same model on a sufficiently large amount of different data sets from the same population and average the results, the average model would be asymptotically equal to the true relationship in the population. In fact, the Gauss-Markov Theorem states that, among all linear _unbiased_ estimators, the OLS estimator has the smallest variance and, therefore, the smallest Mean Squared Error.

Although the OLS estimates can be considered as unbiased when the correct parametric model is selected, in reality there can be a biased estimator with better prediction performance[ESLII]. Since the prediction error is a mixture of the bias and the variance of the model, we ultimately want to achieve the best balance between the two.

In Lasso and Ridge, we exploit the loss function by integrating a penalty term for each parameter estimate that is related to its absolute value. More specifically, parameter estimates that are higher in absolute value result in more loss because of their penalty term, thus leading to the need of an equilibrium between avoiding penalisation and achieving the closest fit. Consequently, the use of penalties constrains the magnitude of the estimates and biases them towards 0, making them less variable. Essentially, we try to strike the correct balance of bias and variance by introducing a small amount of bias in exchange to much less variance.

The loss function of the regularised regression has the following form:
$$L(\vec{\theta}) = \sum_{i=1}^n{[y_i - (\theta_0 + \vec{\theta}\cdot \vec{x_i})]^2} + \lambda \cdot r(\vec{\theta}), ~~~~~\vec{\theta}=(\theta_1,\theta_2,...,\theta_p), ~~\theta_0: intercept$$
, where the sum is the Residual Sum of Squares, $r(\vec{\theta})$ is the penalty function of the regression parameters and $\lambda$ is a hyperparameter called _regularisation parameter_ that adjusts the weight of the penalty and, therefore, the amount of constraint imposed on the parameter estimates.

The difference between the two regularised regression methods is in their penalty function:

* Ridge regression uses the squared $L_2-norm$, $r(\vec{\theta})=\sum_{j=1}^p{\theta_j^2}$, of the parameters' vector, while 
* Lasso uses the $L_1-norm$, $r(\vec{\theta})=\sum_{j=1}^p{\lvert \theta_j \rvert}$. 

This small difference results in a different behaviour. As $\lambda$ increases, Ridge regression will start applying a significant penalty on the parameters, forcing them towards 0. However, as mentioned in [An Introduction to Statistical Learning], unless $\lambda$ is extremely large ($\lambda \to \infty$), Ridge will never actually remove any of the predictors in the model(compute a 0 coefficient estimate). On the other hand, Lasso is able to draw some of the coefficients exactly to 0 when $\lambda$ is sufficiently large and, therefore, it can be considered as a biased regression method that can perform both coefficient penalisation and continuous variable selection. For that reason, we can see Ridge regression as a smoothing technique that decreases model variance, while Lasso can be used for performing flexible variable selection.

According to the above perspective, we wish to apply Ridge regression on the best subset model [4.2.1](#VSModel) to assess whether a reduction in variance can lead to more improvement, meaning an even lower $\hat{MSPE}$. Additionally, we would like to try Lasso on the full model to check if the resulting flexible variable selection demonstrates greater performance than the discrete best-subset method. 


```{r}
# folds vector represents our split of the training data into 5 folds
folds <- numeric(450)
for(i in 1:450) {
  for(j in seq_along(splits)){
    if(i %in% splits[[j]]) {
      folds[i] <- j
      break
    }
  }
}

lambdaGrid <- 10^seq(16, -3, length=20000)
```


However, before we apply either Ridge or Lasso, it is important to tune the hyperparameter $\lambda$ in both cases. In order to ensure the fairness of the results, we perform 5-fold CV on the same 5 folds we used in best-subset selection. Similarly to the approach presented in [An Introduction to Statistical Learning], we calculate the cross-validated average $\hat{MSPE}$ on a grid of $\lambda=10^i$, where i takes 40000 equidistant values from -3 to 16. Then, we select the value of $\lambda$ that yielded the smallest $\hat{MSPE}$.


```{r FindLambdaCVRidge}
f <- formula(price ~ precip*bath)
X <- model.matrix(f, data=trainingSet)[,-1]
cvfitRidge <- cv.glmnet(X, trainingSet$price, alpha=0, nfolds=length(splits), type.measure="mse", foldid=folds, thresh=1e-12, maxit=10^8, lambda = lambdaGrid)
```

Regarding Ridge on the best-subset model [4.2.1](#VSModel), we obtain the smallest $\hat{MSPE} =$ `r min(cvfitRidge$cvm)` when $\lambda =$ `r cvfitRidge$lambda.min`, which is indeed better than the $\hat{MSPE}$ obtained from simply performing best-subset selection. The left plot in figure \ref{fig:chooseLambda} shows the change in the estimated $\hat{MSPE}$ as we gradually depart from the OLS results(that is, when $\lambda \to 0$) by increasing $\lambda$. Although there is a dip shown in the plot, this dip is not prominent and the relative difference in the cross-validated $\hat{MSPE}$ between the best-subset model before and after Ridge is only $\frac{\hat{MSPE_{bestSubset}} - \hat{MSPE_{Ridge}}}{\hat{MSPE_{bestSubset}}} \times 100 =$ `r 100*(calculateMSPECV(variablesSelectionResults$bestModel, trainingSet, "price", splits) - min(cvfitRidge$cvm))/calculateMSPECV(variablesSelectionResults$bestModel, trainingSet, "price", splits)` $\%$. This suggests that the Ridge regression has only a small effect on model [4.2.1](#VSModel) and that the least squares solution seems already adequate [introduction to statistical learning].

```{r, FindLambdaCVLasso}
f <- formula(price ~ precip*bath + dist_am1*parking + dist_am2 + sqft + elevation)
X <- model.matrix(f, data=trainingSet)[,-1]
cvfitLasso <- cv.glmnet(X, trainingSet$price, alpha=1, nfolds=length(splits), type.measure="mse", foldid=folds, thresh=1e-12, maxit=10^8, lambda = lambdaGrid)
```

In terms of applying Lasso on the full model, $\lambda =$ `r cvfitLasso$lambda.min` results in the lowest possible $\hat{MSPE}$ of `r min(cvfitLasso$cvm)`. However, in the case of the Lasso it is common to use the _"one standard error"_ rule. According to this rule, we select the least complex model possible(the largest value of $\lambda$) that yields an $\hat{MSPE}$ value which is no larger than one estimated standard error away of the minimum. In our case, the largest possible $\lambda$ is `r cvfitLasso$lambda.1se` which results in $\hat{MSPE} =$ `r cvfitLasso$cvm[cvfitLasso$lambda == cvfitLasso$lambda.1se]`.

In our research, we assess both cases of Lasso regression. Table \ref{tab:LassoVM} shows the coefficient estimates from performing Lasso on the full model with both choices of $\lambda$. Since we are interested in using Lasso mainly as a variable selection method, we will use its results only to exclude variables from the full model. Therefore, the minimum $\lambda$ results indicate merely the removal of the interaction between _"dist\_am1"_ and _"parking"_, while the ones we obtain from using the 1se $\lambda$ suggest the additional removals of _"elevation"_, _"dist\_am2"_ and _"dist\_am1"_. 

Assessing the full model when we exclude the variables suggested by Lasso in both cases, we have that the minimum $\lambda$ leads to a value of $\hat{MSPE} =$ `r calculateMSPECV(c("parking", "precip*bath", "dist_am1", "dist_am2", "sqft", "elevation"), trainingSet, "price", splits)` and 1se $\lambda$ results in `r calculateMSPECV(c("parking", "precip*bath", "sqft"), trainingSet, "price", splits)`. We observe that the cross-validated $\hat{MSPE}$ is worse than the one obtained from the best-subset model and we point out that applying Ridge on those models does not improve these results.

```{r, fig.align='center', out.width="0.49\\linewidth", fig.show='hold', fig.cap="\\label{fig:chooseLambda} The change in the estimated MSPE as we increase the regularisation parameter, $\\lambda$. The dashed vertical line corresponds to the chosen $\\lambda$ for each regularised regression method, that yields the best CV MSPE. The solid vertical line corresponds to the largest value of $\\lambda$ such that the resulted $\\hat{MSPE}$ is within 1 estimated standard error of the minimum $\\hat{MSPE}$."}
plot(c(10^-3, 500), c(min(cvfitRidge$cvm)-10^5, max(cvfitRidge$cvm[cvfitRidge$lambda <= 500])), type="n", ylab="Cross-validation MSPE", xlab=expression(lambda), main="Ridge on the best-subset model")
lines(cvfitRidge$lambda, cvfitRidge$cvm)
abline(v=cvfitRidge$lambda.min, col="red", lty=2)

plot(c(10^-3, 4000), c(min(cvfitLasso$cvm)-10^5, max(cvfitLasso$cvm[cvfitLasso$lambda <= 4000])), type="n", ylab="Cross-validation MSPE", xlab=expression(lambda), main="Lasso on the full model")
lines(cvfitLasso$lambda, cvfitLasso$cvm)
abline(v=cvfitLasso$lambda.min, col="red", lty=2)
abline(v=cvfitLasso$lambda.1se, col="red", lty=1)
```

```{r LassoVS}
Lasso_minVM <- predict(glmnet(X, trainingSet$price, alpha=1, lambda=lambdaGrid, maxit=10^8, thresh=1e-12), type="coefficients",s= cvfitLasso$lambda.min, exact=TRUE)[,1]

Lasso_1seVM <- predict(glmnet(X, trainingSet$price, alpha=1, lambda=lambdaGrid, maxit=10^8, thresh=1e-12), type="coefficients",s= cvfitLasso$lambda.1se, exact=TRUE)[,1]

LassoVMTable <- data.frame(Min=Lasso_minVM, oneSe=Lasso_1seVM)
LassoVMTable %>%
  kable(col.names=c("Estimates with min $\\lambda$", "Estimates with 1se $\\lambda$"),
        caption="\\label{tab:LassoVM} The parameter estimates from performing Lasso regression on the full model using the training data.")
```

TODO: Change lambda grid from 20000 to 40000

# Conclusions and Further Work

Our goal throughout this paper was to find the best possible regression model to predict the sale price of houses using the rest of their available attributes. First, we explored our data where we detected and removed an extreme outlier and we uncovered the significant importance of the number of bathrooms a house has on its final sale price. Then, we assessed the relationships between our variables that revealed a possibly significant interaction between the number of bathrooms and the precipitation of a house and forced us to deal with the problem of multicollinearity. Lastly, we formalised the results of our analysis by constructing statistical models and performing variable selection and regularised regression to search for the best predictive model.

According to our findings, we conclude that the best regression model is the best-subset model [4.2.1](#VSModel) that consists of the interaction between the precipitation of a house and the amount of bathrooms it has. Specifically, we found out that as the number of bathrooms increases so does, on average, the sale price and, in fact, the house moves up to a higher price category. As for houses with the same amount of bathrooms, our model predicts that, on average, the price drops with precipitation for houses with 1 bathroom, while it increases when the number of bathrooms is 2 or greater. A more detailed interpretation of the final fitted model has already been provided in subsection \ref{BestSubset}. We report that the expected prediction performance of the best model in terms of its MSPE is `r calculateMSPE(selectedModel, testSet, "price")`.

Finally, we would like to iterate on the ways we believe our work could be extended. At first, there are alternative approaches to the way we handled certain problems. For example, one could use Principal Component Regression by applying PCA on the distance variables to eliminate multicollinearity or the hybrid method of elastic nets instead of the independent use of Ridge and Lasso for performing regularised regression.

Further work:
1. More variables
2. Go beyond regression models:

  * PCA on distance variables to counter multicolinearity and regression on the 
    full model with the PC instead of distances
  * Elastic nets
  * k-NN and calculate average neighbours(but first, feature scaling and possibly weighted distance).

~~ TODO ~~

# References
