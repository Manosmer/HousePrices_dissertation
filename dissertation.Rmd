---
title: "Dissertation"
author: "Emmanouil Mertzianis"
date: "10/1/2021"
output:
  pdf_document:
    latex_engine: xelatex
    fig_caption: yes
    number_sections: yes
    toc: true
    toc_depth: 2
    includes:
      in_header: my_header.tex
fontsize: 10pt
bibliography: references.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
```

```{r libraries}
library(knitr)

library(skimr)
library(tidyverse)
library(moderndive)
library(GGally)
library(corrplot)
```



\newpage

# Introduction {#Intro}


## The real estate market and the importance of predictive and descriptive models

## The purpose of the paper

## The structure of the paper
~~ TODO ~~



# Literature {#lit}
~~ TODO ~~


# Exploratory Data Analysis {#EDA}
```{r loadData}
houseprices <- read.csv("~/Desktop/Glasgow Project/housing.csv")

houseprices$bath <- as.factor(houseprices$bath)
houseprices$parking <- as.factor(houseprices$parking)
```

Before we start searching for the best regression model through formal data analysis and model fitting, it is important to explore our data through numerical and graphical summaries. This will allow for a better understanding of the patterns in and the structure of our data and it will enable us to make educated decisions during model fitting. For this purpose, we start by exploring each variable individually and, then, we focus on the relationships between the variables with emphasis on the ones related to the sale price, which is the variable of interest.


## Exploring variables individually
```{r skimdata}
skimmed <- houseprices %>% 
  skimr::skim() %>%
  skimr::focus("# missing"=n_missing, "Unique lvls"=factor.n_unique, Mean=numeric.mean, "SD"=numeric.sd, Min=numeric.p0, "25%"=numeric.p25, Median=numeric.p50, "75%"=numeric.p75, Max=numeric.p100)


# counts for the categorical variables
bathfreqTable <- houseprices %>%
  count(bath) %>%
  spread(key="bath", value="n")

bathfreq <- ""
for(bathnum in colnames(bathfreqTable)) {
  if(bathfreq == "") {
    bathfreq <- paste(bathnum,bathfreqTable[bathnum], sep=": ")
  } else {
    bathfreq <- paste(bathfreq, paste(bathnum,bathfreqTable[bathnum], sep=": "), sep=", ")
  }
}

parkingfreqTable <- houseprices %>%
  count(parking) %>%
  spread(key="parking", value="n")

parkingfreq <- ""
for(parkingtype in colnames(parkingfreqTable)) {
  if(parkingfreq == "") {
    parkingfreq <- paste(parkingtype,parkingfreqTable[parkingtype], sep=": ")
  } else {
    parkingfreq <- paste(parkingfreq, paste(parkingtype,parkingfreqTable[parkingtype], sep=": "), sep=", ")
  }
}

## The 2 tables ##
# Factors table
skimr::yank(skimmed, "factor") %>%
  rename("Variable"=skim_variable) %>%
  mutate("Counts"=c(bathfreq, parkingfreq)) %>%
  kable(caption="\\label{tab:statsCatOriginal} Summary statistics for the categorical variables in the initial data set.")

# Numerics table
skimr::yank(skimmed, "numeric") %>%
  rename("Variable"=skim_variable) %>%
  kable(caption="\\label{tab:statsNumOriginal} Summary statistics for the numerical variables in the initial data set.")

```

As a first step, we are interested in the summary statistics of the individual numerical and categorical variables in our data. The tables \ref{tab:statsCatOriginal} and \ref{tab:statsNumOriginal} contain useful statistics about the variables, prior to making any alterations to the original data set. We note that there are no missing values for any of our variables in the data.

Regarding the categorical variables, we observe that there are five and four unique levels for the categorical variables _"bath"_ and _"parking"_, respectively. Table \ref{tab:statsCatOriginal} shows that most of the sale entries refer to houses with two baths or an "open" type parking. However, the most important observation to note here is a single entry with 63 bathrooms, which is exceedingly higher than all the rest observations in our data that are limited to just 4 bathrooms at maximum. Such an observation is likely to be an outlier and the exploratory analysis to follow further underpins this assumption.

Table \ref{tab:statsNumOriginal} shows statistics about the numerical variables. It becomes apparent that the numerical variables are measured in different numerical scales with differences in the magnitude of their values. In terms of magnitude and standard deviation in ascending order:

* _"elevation"_ presents the smallest values that do not exceed the value of 47 and the smallest standard deviation.

* _"precip"_ and _"sqft"_ come second and third, respectively, with the latter having almost double the standard deviation of the former.

* The three variables representing the distance from three chosen amenities (i.e. _dist_am1_, _dist_am2_ and _dist_am3_) exhibit almost the same standard deviation. However, it seems that the $75^{th}$ percentile of _"dist_am1"_ is, relatively close to the $25^{th}$ percentile of _"dist_am2"_, while the $25^{th}$ percentile of _"dist_am3"_ is a bit higher than the median of _"dist_am2"_. This could indicate that, on average, the distance of houses from _"Amenity 1"_ could be significantly smaller than that from _"Amenity 2"_ and equally for the distances of houses from the _"Amenity 2"_ and _"Amenity 3"_.

* The numerical scale and the standard deviation of _"price"_ are the largest among all numerical variables. Also, it is interesting to point out that there exists a high outlier in _"price"_, even relative to its large magnitude, as it is `r (max(houseprices$price) - mean(houseprices$price))/sd(houseprices$price)` times the standard deviation greater than the mean value. The boxplot in figure \ref{priceBoxplot} further illustrates the extreme outlier in _"price"_.

```{r priceExtremeOutlierBoxplot, out.width="50%", fig.align='center', fig.cap="\\label{priceBoxplot} Boxplot of sale price."}
boxplot(houseprices$price, xlab="Sale Price")
```


Table \ref{tab:ExtremeOutlier} focuses on the aforementioned extreme outlier in _"price"_. The table reveals that the outlier (that is, the `r which.max(houseprices$price)`$^{th}$ observation) contains extreme values in other variables as well. Specifically, that same observation is the one related to the 63 bathrooms, which we have already noted as a possible extreme value, and through further exploration we can show that its value of `r max(houseprices$sqft)` square feet is also extremely high. Those findings suggest that observation `r which.max(houseprices$price)` could have probably come from a different population compared to the rest of the observations in the data. In any case, we lack enough data between this extreme observation and the rest ones, to the point that any model fitting with this outlier included would result in speculating after some range of values and would probably lead to a heavily influenced model. Therefore, we conclude that **we have enough evidence to support our decision on removing observation `r which.max(houseprices$price)` before we move on any further**.


```{r extremeOutlier}
houseprices[which.max(houseprices$price),] %>%
  kable(caption="\\label{tab:ExtremeOutlier} The extreme observation in the variable _'price'_.")
```

```{r}
# removing the extreme high outlier from the data set
houseprices <- houseprices[-which.max(houseprices$price),]
```



```{r skimOutlierRemoved}
skimmed <- houseprices %>% 
  skimr::skim() %>%
  skimr::focus("# missing"=n_missing, Mean=numeric.mean, "SD"=numeric.sd, Min=numeric.p0, "25%"=numeric.p25, Median=numeric.p50, "75%"=numeric.p75, Max=numeric.p100)

# Numerics table
skimr::yank(skimmed, "numeric") %>%
  rename("Variable"=skim_variable) %>%
  kable(caption="\\label{tab:statsNumOutlierRemoved} Summary statistics for the numerical variables after removing the extreme outlier.")

```

After removing the outlier, our conclusions about the variables _"elevation"_, _"dist_am1"_, _"dist_am2"_, _"dist_am3"_ and _"precip"_ are similar to the ones we derived earlier. However, we observe a significant drop in the maximum value of _"sqft"_ along with a significant decrease in its standard deviation, which has now become relatively close to that of _"precip"_. Also, the maximum value and the standard deviation of _"price"_ incurred a large drop.

The boxplots in figure \ref{fig:boxplotAllOutlierRemoval} present graphically the already discussed differences in the magnitude and the variation between the numerical variables, by gradually removing variables from plot to plot. Interestingly, the boxplots suggest that the sample distributions of all numerical variables are fairly symmetrical as we observe the median to lie almost at the middle of the IQR box and roughly equal tails at the top and the bottom. This observation is backed by the computed numerical statistics, where the median is reported to be quite close to the mean value for every numerical variable. **A closer view using histograms in figure \ref{fig:histogramsAllOutlierRemoval} reveals that the sample distribution of all numerical variables in the data resembles that of a sample coming from a Normal Distribution.**



```{r boxdata, fig.align='center', out.width=".49\\linewidth", fig.width=3, fig.height=3, fig.show='hold', fig.cap="\\label{fig:boxplotAllOutlierRemoval} Boxplots on all numerical variables without the extreme outlier."}

houseprices %>%
  select(-bath, -parking) %>%
  gather(key="varname", value = "value") %>%
  ggplot(mapping = aes(x = varname, y = value)) +
  geom_boxplot() +
  xlab(NULL) + ylab("Values") +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))

houseprices %>%
  select(-bath, -parking, -price) %>%
  gather(key="varname", value = "value") %>%
  ggplot(mapping = aes(x = varname, y = value)) +
  geom_boxplot() +
  xlab(NULL) + ylab("Values") + 
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))

houseprices %>%
  select(elevation, precip, sqft) %>%
  gather(key="varname", value = "value") %>%
  ggplot(mapping = aes(x = varname, y = value)) +
  geom_boxplot() +
  xlab(NULL) + ylab("Values") + 
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))
```


```{r histograms, fig.align='center', out.width=".49\\linewidth", fig.width=2, fig.height=2, fig.show='hold', fig.cap="\\label{fig:histogramsAllOutlierRemoval} Histograms on all numerical variables without the extreme outlier. The red dashed line represents the mean value of the variable's values."}

numericalVariables<- houseprices %>%
                      select(-bath, -parking)

# for(colname in colnames(numericalVariables)) {
#   hist(numericalVariables[[colname]], breaks = 80,
#       xlab = colname, main = NULL)
# }

numericalVariables %>%
  ggplot(aes(x = price)) + geom_histogram(bins = 100) + geom_vline(xintercept = mean(numericalVariables$price), colour="red", lty = 2)

numericalVariables %>%
  ggplot(aes(x = sqft)) + geom_histogram(bins = 80) + geom_vline(xintercept = mean(numericalVariables$sqft), colour="red", lty = 2)

numericalVariables %>%
  ggplot(aes(x = elevation)) + geom_histogram(bins = 80) + geom_vline(xintercept = mean(numericalVariables$elevation), colour="red", lty = 2)

numericalVariables %>%
  ggplot(aes(x = precip)) + geom_histogram(bins = 80) + geom_vline(xintercept = mean(numericalVariables$precip), colour="red", lty = 2)

numericalVariables %>%
  ggplot(aes(x = dist_am1)) + geom_histogram(bins = 80) + geom_vline(xintercept = mean(numericalVariables$dist_am1), colour="red", lty = 2)

numericalVariables %>%
  ggplot(aes(x = dist_am2)) + geom_histogram(bins = 80) + geom_vline(xintercept = mean(numericalVariables$dist_am2), colour="red", lty = 2)

numericalVariables %>%
  ggplot(aes(x = dist_am3)) + geom_histogram(bins = 80) + geom_vline(xintercept = mean(numericalVariables$dist_am3), colour="red", lty = 2)

```

\newpage

## Exploring relationships
The primary objective of this paper is to create the most accurate predictive model about _"price"_ using the rest of the available variables in the data. Therefore, it is of interest to explore the relationships of _"price"_ against the other variables. For this purpose, we first explore the relationships of _"price"_ against all other numerical variables and, then, we focus on the categorical variables. Finally, we analyse the relationships of _"price"_ against the numerical variables on different levels of the categorical variables.


### Numerical explanatory variables

Figure \ref{fig:scatterPriceAll} depicts the relationships of _"price"_ against all the rest numerical variables in the form of scatterplots with a simple linear regression line superimposed. From the scatterplots, we observe a random scattering of the data points across all values with no obvious patterns suggesting that there is little association between _"price"_ and any one of the other numerical variables. Additionally, the small slope of the superimposed regression lines along with the little correlation revealed in table \ref{tab:corrplot} show that there is no linear relationship between any of the numerical variables and _"price"_. However, we observe a high correlation between _"dist_am3"_/_"dist_am1"_ and a moderate correlation between _"dist_am1"_/_"dist_am2"_ and _"dist_am2"_/_"dist_am3"_ indicating that there is possibly multicollinearity between the distance variables.


```{r, fig.align='center', out.width="0.49\\linewidth", fig.show='hold', fig.width=2, fig.height=2, fig.cap="\\label{fig:scatterPriceAll} Scatterplots of 'price' against all the rest numerical variables. The simple linear regression line is superimposed on the plots."}

houseprices %>%
  ggplot(mapping = aes(x = dist_am1, y = price)) + 
  geom_point(size = 0.8, alpha=.4, shape = 1) +
  geom_smooth(method = "lm", se = FALSE, lwd=.7)


houseprices %>%
  ggplot(mapping = aes(x = dist_am2, y = price)) + 
  geom_point(size = 0.8, alpha=.4, shape = 1) +
  geom_smooth(method = "lm", se = FALSE, lwd=.7)

houseprices %>%
  ggplot(mapping = aes(x = dist_am3, y = price)) + 
  geom_point(size = 0.8, alpha=.4, shape = 1) +
  geom_smooth(method = "lm", se = FALSE, lwd=.7)

houseprices %>%
  ggplot(mapping = aes(x = elevation, y = price)) + 
  geom_point(size = 0.8, alpha=.4, shape = 1) +
  geom_smooth(method = "lm", se = FALSE, lwd=.7)

houseprices %>%
  ggplot(mapping = aes(x = precip, y = price)) + 
  geom_point(size = 0.8, alpha=.4, shape = 1) +
  geom_smooth(method = "lm", se = FALSE, lwd=.7)

houseprices %>%
  ggplot(mapping = aes(x = sqft, y = price)) + 
  geom_point(size = 0.8, alpha=.4, shape = 1) +
  geom_smooth(method = "lm", se = FALSE, lwd=.7)
```

```{r, out.width="50%", fig.align='center', fig.cap="\\label{tab:corrplot} Correlation between all numerical variables."}
corrplot::corrplot(cor(houseprices %>% select(-bath, -parking)), method="number", type="upper")
```


### Categorical explanatory variables

Figure \ref{fig:boxplotsCat} depicts the sample distribution of _"price"_ for each level of _"bath"_ or _"parking"_. Regarding _"parking"_, we observe quite a significant overlap between the boxplots with small differences between them suggesting that there is little to no difference in the sale price for houses in different _"parking"_ categories. In contrast, we see that there is no overlap between the boxplots of _"price"_ in different _"bath"_ categories with the sale price actually increasing as the number of bathrooms increases. As it can be seen more clearly in figure \ref{fig:histPricebyBath}, the seemingly normally-distributed sample of _"price"_ is completely partitioned based on _"bath"_ into 4 non-overlapping chunks. These findings indicate strongly that the categorical variable _"bath"_ is a significant predictor with a positive relationship with _"price"_.


```{r, out.width=".49\\linewidth", fig.align='center', fig.show='hold', fig.width=3, fig.height=3, fig.cap="\\label{fig:boxplotsCat} Boxplots of 'price' by 'bath' and by 'parking'."}
houseprices %>%
  ggplot(mapping = aes(x = bath, y = price)) +
  geom_boxplot()

houseprices %>%
  ggplot(mapping = aes(x = parking, y = price)) +
  geom_boxplot() + 
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))
```


```{r, fig.align='center', out.width="70%", fig.cap="\\label{fig:histPricebyBath} Histogram of 'price' coloured by 'bath'."}
ggplot(data=houseprices, mapping = aes(x=price, fill=bath)) +
  geom_histogram(bins=100)
```


### Interactions

Finally, it is important to investigate any interactions between categorical and numerical explanatory variables. Figure \ref{fig:scatterPriceAllByBath} shows the relationship of each numerical variable with _"price"_, on each _"bath"_ level. On all plots, we observe four distinct layers of data corresponding to each _"bath"_ level. As we have already explained, these layers represent the full partitioning of the sample distribution of _"price"_ into four non-overlapping chunks when considering the variable _"bath"_. It is apparent that the observations in the data extend across the whole range of values on all numerical explanatory variables on every different _"bath"_ level and that the nature of the relationship between those variabes and _"price"_ stays almost the same regardless of the level of _"bath"_. This indicates that there is no interaction between _"bath"_ and any of the numerical variables and that the additive model is probably appropriate in this case. Furthermore, the relationships with _"price"_ do not seem to improve for any of the numerical variables when we take _"bath"_ into consideration.

Similarly for the categorical variable _"parking"_, it can be shown that there is an almost complete overlap between the layers of data of the different _"parking"_ levels. This suggests that the single regression line model should be suitable for each relationship with _"price"_, with no differences in the slopes or the intercept terms among the categories.


```{r, fig.align='center', out.width="0.49\\linewidth", fig.show='hold', fig.width=4, fig.height=3, fig.cap="\\label{fig:scatterPriceAllByBath} Scatterplots of 'price' against all the rest numerical variables and a simple linear regression line superimposed, coloured by 'bath'."}
houseprices %>%
  ggplot(mapping = aes(x = dist_am1, y = price, colour=bath)) + 
  geom_point(size = 0.8, alpha=.4, shape = 1) +
  geom_smooth(method = "lm", se = FALSE, lwd=.7)

houseprices %>%
  ggplot(mapping = aes(x = dist_am2, y = price, colour=bath)) + 
  geom_point(size = 0.8, alpha=.4, shape = 1) +
  geom_smooth(method = "lm", se = FALSE, lwd=.7)

houseprices %>%
  ggplot(mapping = aes(x = dist_am3, y = price, colour=bath)) + 
  geom_point(size = 0.8, alpha=.4, shape = 1) +
  geom_smooth(method = "lm", se = FALSE, lwd=.7)

houseprices %>%
  ggplot(mapping = aes(x = elevation, y = price, colour=bath)) + 
  geom_point(size = 0.8, alpha=.4, shape = 1) +
  geom_smooth(method = "lm", se = FALSE, lwd=.7)

houseprices %>%
  ggplot(mapping = aes(x = precip, y = price, colour=bath)) + 
  geom_point(size = 0.8, alpha=.4, shape = 1) +
  geom_smooth(method = "lm", se = FALSE, lwd=.7)

houseprices %>%
  ggplot(mapping = aes(x = sqft, y = price, colour=bath)) + 
  geom_point(size = 0.8, alpha=.4, shape = 1) +
  geom_smooth(method = "lm", se = FALSE, lwd=.7)
```


```{r, fig.align='center', fig.cap="\\label{fig:scatterPriceAllByParking} Parking.", eval=FALSE}
houseprices %>%
  ggplot(mapping = aes(x = dist_am1, y = price, colour=parking)) + 
  geom_point(size = 0.8, alpha=1, shape = 1) +
  geom_smooth(method = "lm", se = FALSE, lwd=.7)

houseprices %>%
  ggplot(mapping = aes(x = dist_am2, y = price, colour=parking)) + 
  geom_point(size = 0.8, alpha=.4, shape = 1) +
  geom_smooth(method = "lm", se = FALSE, lwd=.7)

houseprices %>%
  ggplot(mapping = aes(x = dist_am3, y = price, colour=parking)) + 
  geom_point(size = 0.8, alpha=.4, shape = 1) +
  geom_smooth(method = "lm", se = FALSE, lwd=.7)

houseprices %>%
  ggplot(mapping = aes(x = elevation, y = price, colour=parking)) + 
  geom_point(size = 0.8, alpha=.4, shape = 1) +
  geom_smooth(method = "lm", se = FALSE, lwd=.7)

houseprices %>%
  ggplot(mapping = aes(x = precip, y = price, colour=parking)) + 
  geom_point(size = 0.8, alpha=.4, shape = 1) +
  geom_smooth(method = "lm", se = FALSE, lwd=.7)

houseprices %>%
  ggplot(mapping = aes(x = sqft, y = price, colour=parking)) + 
  geom_point(size = 0.8, alpha=.4, shape = 1) +
  geom_smooth(method = "lm", se = FALSE, lwd=.7)
```
  


# Model Fitting: Selecting the best possible regression model {#ModelFitting}

The **primary objective** of this paper is to search for and design the best possible _predictive_ regression model to accurately predict the sale price of a house based on the rest of its attributes. The main approach we chose for this problem is **variable selection on the full model via backward elimination**. 

For this purpose, we decided on assessing our models' performance and conduct variable selection based on the validation (mean squared)prediction error:

$$ \hat{MSPE} = \frac{1}{n} \sum_{i=1}^n{(y_i - \hat{y_i})^2}$$

We achieve this by splitting our original data set into three subsets; training, validation and test set. These sets will be used for training(fitting) the models, assessing their prediction performance(out-of-sample prediction) and calculating the expected prediction performance on the best model, respectively. The split we chose to use is:

* 299 observations for training, 
* 150 observations for assessment and
* 50 observations for reporting the final expected prediction performance.

```{r datasetSplitting}
set.seed(1635863590)
trainingIndices <- sample(1:499, 299)
validationIndices <- sample((1:499)[-trainingIndices], 150)
testIndices <- (1:499)[c(-trainingIndices, -validationIndices)]

trainingSet <- houseprices[trainingIndices,]
validationSet <- houseprices[validationIndices,]
testSet <- houseprices[testIndices,]
```


Finally, we will consider moving beyond linear models by smoothing the best regression model we found using Ridge regression on its subset of predictors and see whether we can improve the prediction performance even further. Also, we will investigate whether variable selection via Lasso regression yields a more powerful model than backward elimination.


## Fitting the full model
In section \ref{EDA}, we discussed about the different available variables in our data and we explored numerically and graphically the relationships between them and especially with _"price"_. Based on our findings, we select the full additive model as our full model to start with:

$$ price_i =  \beta_0 + \beta_1\cdot{\mathbb{I}bath2_i} + \beta_2\cdot{\mathbb{I}bath3_i} + \beta_3\cdot{\mathbb{I}bath4_i}$$
$$\beta_4\cdot{\mathbb{I}parkingNoParking_i} + \beta_5\cdot{\mathbb{I}parkingNotProvided_i} + \beta_6\cdot{\mathbb{I}parkingOpen_i} + $$
$$\beta_7\cdot{sqft_i} + \beta_8\cdot{precip_i} + \beta_9\cdot{elevation_i} + \beta_{10}\cdot{dist\_am1_i} + \beta_{11}\cdot{dist\_am2_i} + \beta_{12}\cdot{dist\_am3_i} + \epsilon_i$$
$$,\epsilon_i\sim N(0, \sigma^2)$$
, where the indicator variables(starting with $\mathbb{I}$) are equal to $1$ when the $i^{th}$ observation's corresponding categorical variable takes the relevant value and $0$ otherwise. The baseline categories are _1_ for _"bath"_ and _Covered_ for _"parking"_.


Table \ref{tab:FullModelFit} shows the OLS estimates of the full model's regression coefficients along with their p-value and 95% Confidence Interval(C.I.), when fitted on the training set. The reported p-values suggest that the coefficients for the categories of _"bath"_ are significant, considering all predictors we used in the model and choosing a significance level of 5%. Furthermore, the absence of overlap between the 95% C.Is of the  _"bath"_ categories further underlines the importance of this predictor. Also, we observe a small p-value for the categories "No Parking" and "Open" of the variable _"parking"_. All the rest estimates have a p-value that is greater than the chosen significance level or, equivalently, a 95% C.I. that includes the value of $0$ as a plausible value for the coefficient in the population's regression model. 

```{r}
fullModel <- lm(price ~ ., data = trainingSet)
get_regression_table(fullModel) %>%
  select(term, estimate, p_value, lower_ci, upper_ci) %>%
  kable(caption = "\\label{tab:FullModelFit} The estimated regression coefficients along with the corresponding p-values and 95% Confidence Intervals from fitting the full additive model on the training set.")
```

Lastly, we can calculate the mean squared prediction error of the full model from predicting on the validation set, which is equal to $\hat{MSPE}=$ `r sum( (validationSet$price - predict(fullModel, validationSet))^2 )/nrow(validationSet)`. This metric will be used for performing variable selection as described in the following subsection.



## Variable Selection



## Regularised regression with Ridge and Lasso


~~ TODO ~~

# Conclusions
~~ TODO ~~

# Further Work
go beyond regression models, k-NN maybe. 
~~ TODO ~~

# References
